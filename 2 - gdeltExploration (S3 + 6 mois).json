{"paragraphs":[{"text":"%md\n## Exploration des donnees GDELT via Spark\nDans ce notebook nous allons commencer a explorer les donnees GDELT qu'on a stoque sur S3","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Exploration des donnees GDELT via Spark</h2>\n<p>Dans ce notebook nous allons commencer a explorer les donnees GDELT qu&rsquo;on a stoque sur S3</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083131_1605837613","id":"20181212-102323_67420128","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3422"},{"text":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport sqlContext.implicits._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.sql.SQLContext\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport sqlContext.implicits._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.sql.SQLContext\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions\n"}]},"apps":[],"jobName":"paragraph_1581022083131_-977850097","id":"20200123-210316_1992644199","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3423"},{"text":"//val PROVIDER = \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\"\nval AWS_ID = \"###\"\nval AWS_KEY = \"###\"\nval AWS_TOKEN= \"###\"\n\n// la classe AmazonS3Client n'est pas serializable\n// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n//@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN))\n//@transient val awsClient = new AmazonS3Client(new BasicAWSCredentials(AWS_ID, AWS_KEY,))/\n\n//sc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", PROVIDER) //   pour AWSeducate , retirer si compte normal\n//sc.hadoopConfiguration.set(\"fs.s3a.awsAccesKeyId\", AWS_ID) // mettre votre ID du fichier credentials.csv\n\n\n\nsc.hadoopConfiguration.set(\"fs.s3.maxConnections\", \"1000\")\nsc.hadoopConfiguration.set(\"fs.s3a.maxConnections\", \"3000\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\n//sc.hadoopConfiguration.set(\"fs.s3a.awsSecretAccessKey\", AWS_KEY) // mettre votre secret du fichier credentials.csv\n//sc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN) //   pour AWSeducate , retirer si compte normal\n\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"AWS_ID: String = ###\nAWS_KEY: String = ###\nAWS_TOKEN: String = ###\n"}]},"apps":[],"jobName":"paragraph_1581022083131_-1440970601","id":"20171217-230735_1688540039","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3424"},{"text":"%md\n### EVENT","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>EVENT</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083131_-1424667544","id":"20200118-170725_2018393531","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3425"},{"text":"%md Les fichiers sont stoquees compresses, on a besoin d'un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu'on les lit depuis S3:","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les fichiers sont stoquees compresses, on a besoin d&rsquo;un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu&rsquo;on les lit depuis S3:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083131_1397228709","id":"20181212-102329_808049084","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3426"},{"text":"//#################################################\n// LOADING 'EVENT' TABLE AS PER GDELT COLUMNS TABLE \n//#################################################\n\n//#############################################################################\n//  ### NOTES :  Remplacer s3a par s3  dans les liens pour acceder aux buckets\n//#############################################################################\n\n//exporter fichiers issus de masterfilelist.txt\nval textRDD_eng = sc.binaryFiles(\"s3://hiroto-yamakawa-gdelt2020/2019[0-9]*.export.CSV.zip\"). // charger quelques fichers via une regex   //replace by 201912[0-9]* for the December 2019 only and 20191201[0-9]* for 1 day\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{ case null => zis.close(); false\n            case _ => true }.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n    \n//exporter fichiers issus de masterfilelist-translation.txt\nval textRDD_translate = sc.binaryFiles(\"s3://hiroto-yamakawa-gdelt2020/2019[0-9]*.translation.export.CSV.zip\"). \n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{ case null => zis.close(); false\n            case _ => true }.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n    \n\n//concaténer les RDD et transformer en DataFrame                           //Pour des raisons de cohérence, nous indiquons ici le nom des colonnes comme indiqué dans la documentation GDELT. Nous les renommerons lorsque nécessaire plus bas\nval eventDF = textRDD_eng.union(textRDD_translate).toDF()\n.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n.withColumn(\"Day\", split($\"value\", \"\\\\t\").getItem(1))\n.withColumn(\"MonthYear\", split($\"value\", \"\\\\t\").getItem(2))\n.withColumn(\"Year\", split($\"value\", \"\\\\t\").getItem(3))\n.withColumn(\"FractionDate\", split($\"value\", \"\\\\t\").getItem(4))\n.withColumn(\"Actor1Code\", split($\"value\", \"\\\\t\").getItem(5))\n.withColumn(\"Actor1Name\", split($\"value\", \"\\\\t\").getItem(6))\n.withColumn(\"Actor1CountryCode\", split($\"value\", \"\\\\t\").getItem(7))\n.withColumn(\"Actor1KnownGroupCode\", split($\"value\", \"\\\\t\").getItem(8))\n.withColumn(\"Actor1EthnicCode\", split($\"value\", \"\\\\t\").getItem(9))\n.withColumn(\"Actor1Religion1Code\", split($\"value\", \"\\\\t\").getItem(10))\n.withColumn(\"Actor1Religion2Code\", split($\"value\", \"\\\\t\").getItem(11))\n.withColumn(\"Actor1Type1Code\", split($\"value\", \"\\\\t\").getItem(12))\n.withColumn(\"Actor1Type2Code\", split($\"value\", \"\\\\t\").getItem(13))\n.withColumn(\"Actor1Type3Code\", split($\"value\", \"\\\\t\").getItem(14))\n.withColumn(\"Actor2Code\", split($\"value\", \"\\\\t\").getItem(15))\n.withColumn(\"Actor2Name\", split($\"value\", \"\\\\t\").getItem(16))\n.withColumn(\"Actor2CountryCode\", split($\"value\", \"\\\\t\").getItem(17))\n.withColumn(\"Actor2KnownGroupCode\", split($\"value\", \"\\\\t\").getItem(18))\n.withColumn(\"Actor2EthnicCode\", split($\"value\", \"\\\\t\").getItem(19))\n.withColumn(\"Actor2Religion1Code\", split($\"value\", \"\\\\t\").getItem(20))\n.withColumn(\"Actor2Religion2Code\", split($\"value\", \"\\\\t\").getItem(21))\n.withColumn(\"Actor2Type1Code\", split($\"value\", \"\\\\t\").getItem(22))\n.withColumn(\"Actor2Type2Code\", split($\"value\", \"\\\\t\").getItem(23))\n.withColumn(\"Actor2Type3Code\", split($\"value\", \"\\\\t\").getItem(24))\n.withColumn(\"IsRootEvent\", split($\"value\", \"\\\\t\").getItem(25))\n.withColumn(\"EventCode\", split($\"value\", \"\\\\t\").getItem(26))\n.withColumn(\"EventBaseCode\", split($\"value\", \"\\\\t\").getItem(27))\n.withColumn(\"EventRootCode\", split($\"value\", \"\\\\t\").getItem(28))\n.withColumn(\"QuadClass\", split($\"value\", \"\\\\t\").getItem(29))\n.withColumn(\"GoldsteinScale\", split($\"value\", \"\\\\t\").getItem(30))\n.withColumn(\"NumMentions\", split($\"value\", \"\\\\t\").getItem(31))\n.withColumn(\"NumSources\", split($\"value\", \"\\\\t\").getItem(32))\n.withColumn(\"NumArticles\", split($\"value\", \"\\\\t\").getItem(33))\n.withColumn(\"AvgTone\", split($\"value\", \"\\\\t\").getItem(34))\n.withColumn(\"Actor1Geo_Type\", split($\"value\", \"\\\\t\").getItem(35))\n.withColumn(\"Actor1Geo_FullName\", split($\"value\", \"\\\\t\").getItem(36))\n.withColumn(\"Actor1Geo_CountryCode\", split($\"value\", \"\\\\t\").getItem(37))\n.withColumn(\"Actor1Geo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(38))\n.withColumn(\"Actor1Geo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(39))\n.withColumn(\"Actor1Geo_Lat\", split($\"value\", \"\\\\t\").getItem(40))\n.withColumn(\"Actor1Geo_Long\", split($\"value\", \"\\\\t\").getItem(41))\n.withColumn(\"Actor1Geo_FeatureID\", split($\"value\", \"\\\\t\").getItem(42))\n.withColumn(\"Actor2Geo_Type\", split($\"value\", \"\\\\t\").getItem(43))\n.withColumn(\"Actor2Geo_FullName\", split($\"value\", \"\\\\t\").getItem(44))\n.withColumn(\"Actor2Geo_CountryCode\", split($\"value\", \"\\\\t\").getItem(45))\n.withColumn(\"Actor2Geo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(46))\n.withColumn(\"Actor2Geo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(47))\n.withColumn(\"Actor2Geo_Lat\", split($\"value\", \"\\\\t\").getItem(48))\n.withColumn(\"Actor2Geo_Long\", split($\"value\", \"\\\\t\").getItem(49))\n.withColumn(\"Actor2Geo_FeatureID\", split($\"value\", \"\\\\t\").getItem(50))\n.withColumn(\"ActionGeo_Type\", split($\"value\", \"\\\\t\").getItem(51))\n.withColumn(\"ActionGeo_FullName\", split($\"value\", \"\\\\t\").getItem(52))\n.withColumn(\"ActionGeo_CountryCode\", split($\"value\", \"\\\\t\").getItem(53))\n.withColumn(\"ActionGeo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(54))\n.withColumn(\"ActionGeo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(55))\n.withColumn(\"ActionGeo_Lat\", split($\"value\", \"\\\\t\").getItem(56))\n.withColumn(\"ActionGeo_Long\", split($\"value\", \"\\\\t\").getItem(57))\n.withColumn(\"ActionGeo_FeatureID\", split($\"value\", \"\\\\t\").getItem(58))\n.withColumn(\"DATEADDED\", split($\"value\", \"\\\\t\").getItem(59))\n.withColumn(\"SOURCEURL\", split($\"value\", \"\\\\t\").getItem(60))\n.drop($\"value\")\n\n","user":"anonymous","dateUpdated":"2020-02-06T20:54:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"textRDD_eng: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at flatMap at <console>:64\ntextRDD_translate: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at flatMap at <console>:78\neventDF: org.apache.spark.sql.DataFrame = [GLOBALEVENTID: string, Day: string ... 59 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083131_-1887019717","id":"20171217-232457_1732696781","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3427"},{"text":"%md\n### GLOBAL KNOWLEDGE GRAPH","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>GLOBAL KNOWLEDGE GRAPH</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083132_-1659032762","id":"20200118-170130_892217168","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3428"},{"text":"//exporter fichiers issus de masterfilelist.txt\nval gkgRDD_eng = sc.binaryFiles(\"s3://hiroto-yamakawa-telecom-gdelt2019/2019[0-9]*.gkg.csv.zip\"). // charger quelques fichers via une regex   \n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{ case null => zis.close(); false\n            case _ => true }.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n\n//exporter fichiers issus de masterfilelist-translation.txt\nval gkgRDD_translate = sc.binaryFiles(\"s3://hiroto-yamakawa-telecom-gdelt2019/2019[0-9]*.translation.gkg.csv.zip\").  \n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{ case null => zis.close(); false\n            case _ => true }.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n    \n//concaténer les RDD et transformer en DataFrame                                    \nval gkgDF = gkgRDD_eng.union(gkgRDD_translate).toDF()\n.withColumn(\"GKGRECORDID\", split($\"value\", \"\\\\t\").getItem(0))\n.withColumn(\"DATE\", split($\"value\", \"\\\\t\").getItem(1))\n.withColumn(\"SourceCollectionIdentifier\", split($\"value\", \"\\\\t\").getItem(2))\n.withColumn(\"SourceCommonName\", split($\"value\", \"\\\\t\").getItem(3))\n.withColumn(\"DocumentIdentifier\", split($\"value\", \"\\\\t\").getItem(4))\n.withColumn(\"Counts\", split($\"value\", \"\\\\t\").getItem(5))\n.withColumn(\"V2Counts\", split($\"value\", \"\\\\t\").getItem(6))\n.withColumn(\"Themes\", split($\"value\", \"\\\\t\").getItem(7))\n.withColumn(\"V2Themes\", split($\"value\", \"\\\\t\").getItem(8))\n.withColumn(\"Locations\", split($\"value\", \"\\\\t\").getItem(9))\n.withColumn(\"V2Locations\", split($\"value\", \"\\\\t\").getItem(10))\n.withColumn(\"Persons\", split($\"value\", \"\\\\t\").getItem(11))\n.withColumn(\"V2Persons\", split($\"value\", \"\\\\t\").getItem(12))\n.withColumn(\"Organizations\", split($\"value\", \"\\\\t\").getItem(13))\n.withColumn(\"V2Organizations\", split($\"value\", \"\\\\t\").getItem(14))\n.withColumn(\"V2Tone\", split($\"value\", \"\\\\t\").getItem(15))\n.withColumn(\"Dates\", split($\"value\", \"\\\\t\").getItem(16))\n.withColumn(\"GCAM\", split($\"value\", \"\\\\t\").getItem(17))\n.withColumn(\"SharingImage\", split($\"value\", \"\\\\t\").getItem(18))\n.withColumn(\"RelatedImages\", split($\"value\", \"\\\\t\").getItem(19))\n.withColumn(\"SocialImageEmbeds\", split($\"value\", \"\\\\t\").getItem(20))\n.withColumn(\"SocialVideoEmbeds\", split($\"value\", \"\\\\t\").getItem(21))\n.withColumn(\"Quotations\", split($\"value\", \"\\\\t\").getItem(22))\n.withColumn(\"AllNames\", split($\"value\", \"\\\\t\").getItem(23))\n.withColumn(\"Amounts\", split($\"value\", \"\\\\t\").getItem(24))\n.withColumn(\"TranslationInfo\", split($\"value\", \"\\\\t\").getItem(25))\n.withColumn(\"Extras\", split($\"value\", \"\\\\t\").getItem(26))\n.drop($\"value\")","user":"anonymous","dateUpdated":"2020-02-06T20:54:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkgRDD_eng: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[330] at flatMap at <console>:71\ngkgRDD_translate: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[332] at flatMap at <console>:85\ngkgDF: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, DATE: string ... 25 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083132_-1772075335","id":"20200118-170907_2089007112","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3429"},{"text":"%md\n### MENTION ","user":"anonymous","dateUpdated":"2020-02-06T20:52:47+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1581022083132_2011610202","id":"20200118-170957_1173975364","dateCreated":"2020-02-06T20:48:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3431","dateFinished":"2020-02-06T20:52:47+0000","dateStarted":"2020-02-06T20:52:47+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>MENTION</h3>\n</div>"}]}},{"text":"//exporter fichiers issus de masterfilelist.txt\nval mentionRDD_eng = sc.binaryFiles(\"s3://hiroto-yamakawa-gdelt2020/2019[0-9]*.mentions.CSV.zip\").    //replace by 201912[0-9]* for the December 2019 only and 20191201[0-9]* for 1 day\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{ case null => zis.close(); false\n            case _ => true }.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n    \n//exporter fichiers issus de masterfilelist-translation.txt\nval mentionRDD_translate = sc.binaryFiles(\"s3://hiroto-yamakawa-gdelt2020/2019[0-9]*.translation.mentions.CSV.zip\"). \n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile{ case null => zis.close(); false\n            case _ => true }.\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\n    \n//concaténer les RDD et transformer en DataFrame\nval mentionDF = mentionRDD_eng.union(mentionRDD_translate).toDF()\n.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n.withColumn(\"EventTimeDate\", split($\"value\", \"\\\\t\").getItem(1))\n.withColumn(\"MentionTimeDate\", split($\"value\", \"\\\\t\").getItem(2))\n.withColumn(\"MentionType\", split($\"value\", \"\\\\t\").getItem(3))\n.withColumn(\"MentionSourceName\", split($\"value\", \"\\\\t\").getItem(4))\n.withColumn(\"MentionIdentifier\", split($\"value\", \"\\\\t\").getItem(5))\n.withColumn(\"SentenceID\", split($\"value\", \"\\\\t\").getItem(6))\n.withColumn(\"Actor1CharOffset\", split($\"value\", \"\\\\t\").getItem(7))\n.withColumn(\"Actor2CharOffset\", split($\"value\", \"\\\\t\").getItem(8))\n.withColumn(\"ActionCharOffset\", split($\"value\", \"\\\\t\").getItem(9))\n.withColumn(\"InRawText\", split($\"value\", \"\\\\t\").getItem(10))\n.withColumn(\"Confidence\", split($\"value\", \"\\\\t\").getItem(11))\n.withColumn(\"MentionDocLen\", split($\"value\", \"\\\\t\").getItem(12))\n.withColumn(\"MentionDocTone\", split($\"value\", \"\\\\t\").getItem(13))\n.withColumn(\"MentionDocTranslationInfo\", split($\"value\", \"\\\\t\").getItem(14))\n.withColumn(\"Extras\", split($\"value\", \"\\\\t\").getItem(15))\n.withColumn(\"Langue_1\", when (col(\"MentionDocTranslationInfo\") === \"\", \":eng\").otherwise(col(\"MentionDocTranslationInfo\")))    /// les lignes suivante concernent la Tramsformation de la colonne MentionDocTranslationInfo\n.withColumn(\"Langue_2\", split($\"Langue_1\", \";\").getItem(0))\n.withColumn(\"Langue\", split($\"Langue_2\", \":\").getItem(1))\n.drop($\"Langue_1\")\n.drop($\"Langue_2\")\n.drop($\"value\")\n//.drop($\"MentionDocTranslationInfo\")  à supprimer une fois le groupe convaincu que la transformation de la colonne fonctionne\n","user":"anonymous","dateUpdated":"2020-02-06T20:51:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mentionRDD_eng: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at flatMap at <console>:53\nmentionRDD_translate: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[23] at flatMap at <console>:67\nmentionDF: org.apache.spark.sql.DataFrame = [GLOBALEVENTID: string, EventTimeDate: string ... 15 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083132_1097961813","id":"20200118-171001_883950930","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3432"},{"text":"mentionDF.select(\"MentionDocTranslationInfo\",\"Langue\").distinct.cache().show()      // piste d'amélioration : trouver ou créer un dico key/value pour remplacer le sigle par la langue full name   . De Mëme pour les pays","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------------+------+\n|MentionDocTranslationInfo|Langue|\n+-------------------------+------+\n|     srclc:dan;eng:GT-...|   dan|\n|     srclc:swe;eng:GT-...|   swe|\n|     srclc:ces;eng:Mos...|   ces|\n|     srclc:spa;eng:Mos...|   spa|\n|     srclc:kat;eng:GT-...|   kat|\n|     srclc:tur;eng:GT-...|   tur|\n|     srclc:urd;eng:GT-...|   urd|\n|     srclc:rus;eng:GT-...|   rus|\n|     srclc:est;eng:Mos...|   est|\n|     srclc:som;eng:GT-...|   som|\n|     srclc:ita;eng:GT-...|   ita|\n|     srclc:nld;eng:GT-...|   nld|\n|     srclc:lit;eng:GT-...|   lit|\n|     srclc:bul;eng:GT-...|   bul|\n|     srclc:ben;eng:GT-...|   ben|\n|     srclc:tel;eng:GT-...|   tel|\n|     srclc:spa;eng:GT-...|   spa|\n|     srclc:hrv;eng:GT-...|   hrv|\n|     srclc:fra;eng:GT-...|   fra|\n|     srclc:fas;eng:GT-...|   fas|\n+-------------------------+------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1581022083132_-737681877","id":"20200118-171218_302028850","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3433"},{"text":"// ##########################################\n// SELECTING USEFUL COLUMNS FOR THE 4 QUERIES\n// ##########################################\n\n\nval eventDF_selected  = eventDF.select(\"GLOBALEVENTID\",\"Day\",\"MonthYear\",\"Year\",\"Actor1CountryCode\",\"Actor2CountryCode\",\"ActionGeo_CountryCode\").cache()\n\nval gkgDF_selected = gkgDF.select(\"DATE\",\"SourceCommonName\",\"DocumentIdentifier\",\"V2Themes\",\"V2Locations\",\"V2Persons\",\"V2Tone\").cache()\n\nval mentionDF_selected = mentionDF.select(\"GLOBALEVENTID\",\"MentionTimeDate\",\"MentionIdentifier\",\"MentionDocTone\",\"Langue\").cache()\n\n//JOIN eventDF_selected with mentionDF_selected ON GLOBALEVENTID   \nval event_mention_DF = eventDF_selected.join(mentionDF_selected,\"GlobalEventID\").cache()       // is the .cache() effective here? TBC\n\n\n\n//val eventDF_selected  = eventDF.select(\"GLOBALEVENTID\",\"Day\",\"MonthYear\",\"Year\",\"Actor1Name\",\"Actor1CountryCode\",\"Actor2Name\",\"Actor2CountryCode\",\"NumMentions\",\"NumArticles\",\"AvgTone\",\"ActionGeo_FullName\",\"ActionGeo_CountryCode\").cache()\n\n//val gkgDF_selected = gkgDF.select(\"GKGRECORDID\",\"DATE\",\"SourceCommonName\",\"DocumentIdentifier\",\"Themes\",\"Locations\",\"Persons\",\"V2Themes\",\"V2Locations\",\"V2Persons\",\"Organizations\",\"V2Tone\",\"TranslationInfo\").cache()\n\n//val mentionDF_selected = mentionDF.select(\"GLOBALEVENTID\",\"EventTimeDate\",\"MentionTimeDate\",\"MentionSourceName\",\"MentionIdentifier\",\"MentionDocTone\",\"MentionDocTranslationInfo\",\"Langue\").cache()","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"eventDF_selected: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [GLOBALEVENTID: string, Day: string ... 5 more fields]\ngkgDF_selected: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [DATE: string, SourceCommonName: string ... 5 more fields]\nmentionDF_selected: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [GLOBALEVENTID: string, MentionTimeDate: string ... 3 more fields]\nevent_mention_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [GLOBALEVENTID: string, Day: string ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083132_-101178865","id":"20200118-171245_1235177400","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3434"},{"text":"%md\n### REQUETES  / QUERIES","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>REQUETES / QUERIES</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083132_-1107101578","id":"20200118-171317_1531636705","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3435"},{"text":"%md\n### 1) afficher le nombre d’articles/évènements qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article).","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>1) afficher le nombre d’articles/évènements qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article).</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083132_-1722717565","id":"20200118-171340_1397000236","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3436"},{"text":"//######################\n// VERSION SQL\n//######################\n\nimport org.apache.spark.sql.SparkSession\n\nval slice = udf((string : String, from : Int, to : Int) => string.slice(from,to))\n\nval eventDF_vf =eventDF.withColumn(\"YEAR\", slice($\"Day\",lit(0),lit(4)))\n.withColumn(\"mois\", slice($\"Day\",lit(4),lit(6)))\n.withColumn(\"jour\", slice($\"Day\",lit(6),lit(8)))\n.withColumn(\"date\", concat_ws(\"/\",$\"YEAR\",$\"mois\",$\"jour\"))\n.withColumn(\"mois1\", concat_ws(\"/\",$\"YEAR\",$\"mois\"))\n.drop($\"Day\")\n\neventDF_vf.createOrReplaceTempView(\"eventDF1\");\nmentionDF.createOrReplaceTempView(\"mentionDF1\");\nval requete1_DF=sql(\"SELECT r.GLOBALEVENTID as evenement,r.date,r.ActionGeo_CountryCode as pays,s.MentionIdentifier as Mention,s.Langue as langue_article FROM eventDF1 r JOIN mentionDF1 s ON r.GLOBALEVENTID=s.GLOBALEVENTID\").cache()\n\n\n\n// Création de plusieurs tables sur cassandra\nval requete1_1 = requete1_DF.groupBy(\"date\", \"pays\", \"langue_article\",\"evenement\").agg(countDistinct($\"Mention\").as(\"nombre_article\")).filter($\"pays\" !==\"\").cache()\n//val requete1_2 = requete1_DF_cast.groupBy(\"Day\",\"GLOBALEVENTID\").agg(countDistinct($\"Mention\").as(\"Nbre_articles\"))\n//val requete1_3 = requete1_DF_cast.groupBy(\"Langue\",\"GLOBALEVENTID\").agg(countDistinct($\"Mention\").as(\"Nbre_articles\"))\n//val requete1_4 = requete1_DF_cast.groupBy(\"Pays\",\"GLOBALEVENTID\").agg(countDistinct($\"Mention\").as(\"Nbre_articles\"))\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nimport org.apache.spark.sql.SparkSession\nslice: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function3>,StringType,Some(List(StringType, IntegerType, IntegerType)))\neventDF_vf: org.apache.spark.sql.DataFrame = [GLOBALEVENTID: string, MonthYear: string ... 62 more fields]\nrequete1_DF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [evenement: string, date: string ... 3 more fields]\nrequete1_1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, pays: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083132_1943435180","id":"20200122-182358_402742949","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3437"},{"text":"//CASSANDRA\nrequete1_1.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question1Df_6mois_babacar.parquet\")","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n  ... 54 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 38.0 failed 4 times, most recent failure: Lost task 19.3 in stage 38.0 (TID 20237, ip-172-31-7-74.ec2.internal, executor 12): java.io.EOFException: Unexpected end of ZLIB input stream\n\tat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n\tat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n\tat java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n  ... 76 more\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\n  at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n  at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n  at java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n  at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n  at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n  at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n  at java.io.InputStreamReader.read(InputStreamReader.java:184)\n  at java.io.BufferedReader.fill(BufferedReader.java:161)\n  at java.io.BufferedReader.readLine(BufferedReader.java:324)\n  at java.io.BufferedReader.readLine(BufferedReader.java:389)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1581022083132_261764509","id":"20200123-101757_1754912705","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3438"},{"text":"//######################\n// VERSION PUR DATAFRAME \n//######################\n\n\n//FONCTION pour couper DATE en Year,month,day\n\nval slice = udf((string : String, from : Int, to : Int) => string.slice(from,to))\n\n\nval question1Df = event_mention_DF                                                                                                                                                                                \n.filter($\"ActionGeo_CountryCode\" !== \"\")\n.groupBy(\"ActionGeo_CountryCode\", \"Day\", \"Langue\",\"GLOBALEVENTID\")\n.agg(countDistinct($\"MentionIdentifier\") as \"distinct_count\") \n.withColumn(\"YEAR\", slice($\"Day\",lit(0),lit(4)))\n.withColumn(\"MONTH\", slice($\"Day\",lit(4),lit(6)))\n.withColumn(\"DAY\", slice($\"Day\",lit(6),lit(8)))\n.withColumn(\"Date\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\",$\"DAY\"))\n.drop($\"YEAR\")\n.drop($\"MONTH\")\n.drop($\"DAY\")\n.drop(\"Day\")\n\n\nval question1DfFinal_ = question1Df.select(\"Date\",\"ActionGeo_CountryCode\",\"Langue\",\"GLOBALEVENTID\",\"distinct_count\")\n.withColumnRenamed(\"Date\", \"date\")\n.withColumnRenamed(\"ActionGeo_CountryCode\", \"pays\")                                                                             //FIPS10-4 Country code dictionnary\n.withColumnRenamed(\"Langue\", \"langue_article\")\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.withColumnRenamed(\"distinct_count\", \"nombre_article\")\n\n\nval question1DfFinal= question1DfFinal_.sort($\"nombre_article\".desc)                                                                                                            \n\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nslice: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function3>,StringType,Some(List(StringType, IntegerType, IntegerType)))\nquestion1Df: org.apache.spark.sql.DataFrame = [ActionGeo_CountryCode: string, Langue: string ... 3 more fields]\nquestion1DfFinal_: org.apache.spark.sql.DataFrame = [date: string, pays: string ... 3 more fields]\nquestion1DfFinal: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, pays: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083133_1227360174","id":"20200119-145501_723585665","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3439"},{"text":"//CASSANDRA\nquestion1DfFinal.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question1Df_6mois.parquet\")\n\n// for 1 month\n//question3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question1Df_1mois.parquet\")","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n  ... 54 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 4 times, most recent failure: Lost task 1.3 in stage 43.0 (TID 20284, ip-172-31-6-54.ec2.internal, executor 14): java.io.EOFException: Unexpected end of ZLIB input stream\n\tat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n\tat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n\tat java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:115)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n  at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n  ... 76 more\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\n  at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n  at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n  at java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n  at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n  at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n  at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n  at java.io.InputStreamReader.read(InputStreamReader.java:184)\n  at java.io.BufferedReader.fill(BufferedReader.java:161)\n  at java.io.BufferedReader.readLine(BufferedReader.java:324)\n  at java.io.BufferedReader.readLine(BufferedReader.java:389)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:115)\n  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:107)\n  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1581022083133_1197451442","id":"20200122-195806_363813514","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3440"},{"text":"//######################\n//## COMMENTAIRES ######\n//######################\n\n// COUNT VS COUNTDISTINCT\n\nval question1Dfbis_countdistinct = event_mention_DF\n.filter($\"ActionGeo_CountryCode\" !== \"\")\n.groupBy(\"ActionGeo_CountryCode\", \"Day\", \"Langue\",\"GLOBALEVENTID\")\n.agg(countDistinct($\"MentionIdentifier\") as \"distinct_count\")   //regarder la difference avec count() ci dessous  \n.sort($\"distinct_count\".desc)\n.show()\n\n\nval question1Dfbis_count = event_mention_DF\n.filter($\"ActionGeo_CountryCode\" !== \"\")\n.groupBy(\"ActionGeo_CountryCode\", \"Day\", \"Langue\",\"GLOBALEVENTID\")\n.count()   \n.sort($\"count\".desc)\n.show()                                                  // on voit que les valeurs de la colonne count est ici beaucoup plus eleve que dans la colonne distinct_count au dessus.  Pourquoi ? Reponse dans la requete ci-dessous\n\n\n//C'est parcequ'un evenement peut etre mentionne plusieurs fois dans un meme article comme le montre la requete suivante, ce qui fausse le resultat \n//il faudra donc faire un count distinct sur \"Mentionidentifier\" pour compter le nombre d'articles et pas le nombres de mentions\n\nval mention_par_event_df = event_mention_DF.groupBy(\"GLOBALEVENTID\",\"MentionIdentifier\").count().sort($\"count\".desc).show() ","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1581022083133_20041951","id":"20200118-171343_128590321","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3441"},{"text":"%md \n### 2) Pour un pays donné en paramètre, affichez les évènements qui y ont eu place triées par le nombre de mentions (tri décroissant); permettez une agrégation par jour/mois/année","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>2) Pour un pays donné en paramètre, affichez les évènements qui y ont eu place triées par le nombre de mentions (tri décroissant); permettez une agrégation par jour/mois/année</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083133_-574076413","id":"20200118-171548_564900489","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3442"},{"text":"//####################\n//### VERSION SQL ####\n//####################\n\nval requete2_DF=sql(\"SELECT r.GLOBALEVENTID as evenement,r.date,r.mois1,r.YEAR as annee,r.ActionGeo_CountryCode as pays,r.MonthYear as Month,r.Year,s.MentionIdentifier as Mention FROM eventDF1 r JOIN mentionDF1 s ON r.GLOBALEVENTID= s.GLOBALEVENTID\")\n\nval requete2 = requete2_DF.groupBy(\"pays\", \"date\",\"mois1\",\"annee\",\"evenement\").agg(count($\"Mention\").as(\"nombre_mentions\")).sort($\"nombre_mentions\".desc).filter($\"pays\" !==\"\").withColumnRenamed(\"mois1\", \"mois_annee\")\n\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nrequete2_DF: org.apache.spark.sql.DataFrame = [evenement: string, date: string ... 6 more fields]\nrequete2: org.apache.spark.sql.DataFrame = [pays: string, date: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083133_384010280","id":"20200123-101916_4373161","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3443"},{"text":"//CASSANDRA\nrequete2.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/requete2_6mois_babacar.parquet\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n  ... 54 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 46.0 failed 4 times, most recent failure: Lost task 19.3 in stage 46.0 (TID 20371, ip-172-31-14-81.ec2.internal, executor 16): java.io.EOFException: Unexpected end of ZLIB input stream\n\tat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n\tat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n\tat java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n  at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n  ... 76 more\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\n  at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n  at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n  at java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n  at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n  at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n  at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n  at java.io.InputStreamReader.read(InputStreamReader.java:184)\n  at java.io.BufferedReader.fill(BufferedReader.java:161)\n  at java.io.BufferedReader.readLine(BufferedReader.java:324)\n  at java.io.BufferedReader.readLine(BufferedReader.java:389)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1581022083133_-450098780","id":"20200123-111703_613669991","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3444"},{"text":"//####################\n//# VERSION DATAFRAME \n//####################\n\nval question2Df= event_mention_DF.select(\"ActionGeo_CountryCode\",\"Day\",\"GLOBALEVENTID\")    // on aurait pu choisir ActionGeo_FullName également\n.filter($\"ActionGeo_CountryCode\" !== \"\")\n.groupBy(\"ActionGeo_CountryCode\",\"Day\",\"GLOBALEVENTID\")\n.count()                                                                                                    //ici on veut le nombre de mentions et pas le nombre d'articles, donc .count() est correct\n.withColumn(\"YEAR\", slice($\"Day\",lit(0),lit(4)))\n.withColumn(\"MONTH\", slice($\"Day\",lit(4),lit(6)))\n.withColumn(\"DAY\", slice($\"Day\",lit(6),lit(8)))\n.withColumn(\"Date\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\",$\"DAY\"))\n.withColumn(\"Date_month\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\"))\n.drop($\"MONTH\")\n.drop($\"DAY\")\n.drop($\"Day\")\n\n\n\nval question2DfFinal= question2Df.select(\"ActionGeo_CountryCode\",\"Date\",\"Date_month\",\"YEAR\",\"GLOBALEVENTID\",\"count\")\n.withColumnRenamed(\"ActionGeo_CountryCode\", \"pays\")\n.withColumnRenamed(\"Date\", \"date\")\n.withColumnRenamed(\"Date_month\", \"mois_annee\")\n.withColumnRenamed(\"YEAR\", \"annee\")\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.withColumnRenamed(\"count\", \"nombre_mentions\")\n\n\n\nquestion2DfFinal.show(1)","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 50.0 failed 4 times, most recent failure: Lost task 1.3 in stage 50.0 (TID 20410, ip-172-31-5-203.ec2.internal, executor 17): java.io.EOFException: Unexpected end of ZLIB input stream\n\tat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n\tat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n\tat java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:115)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n  ... 54 elided\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\n  at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n  at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n  at java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n  at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n  at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n  at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n  at java.io.InputStreamReader.read(InputStreamReader.java:184)\n  at java.io.BufferedReader.fill(BufferedReader.java:161)\n  at java.io.BufferedReader.readLine(BufferedReader.java:324)\n  at java.io.BufferedReader.readLine(BufferedReader.java:389)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:115)\n  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:107)\n  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1581022083134_740838998","id":"20200118-171548_1114384575","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3445"},{"text":"//CASSANDRA\nquestion2DfFinal.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question2Df_6mois.parquet\")\n\n// for 1 month\n//question3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question2Df_1mois.parquet\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:53: error: not found: value question2DfFinal\n       question2DfFinal.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question2Df_6mois.parquet\")\n       ^\n"}]},"apps":[],"jobName":"paragraph_1581022083134_-1268316517","id":"20200118-171548_1559068102","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3446"},{"text":"%md\n### 3) Pour une source de donnés passée en paramètre (gkg.SourceCommonName) affichez les thèmes, personnes, lieux dont les articles de cette sources parlent ainsi que le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permettez une agrégation par jour/mois/année.","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>3) Pour une source de donnés passée en paramètre (gkg.SourceCommonName) affichez les thèmes, personnes, lieux dont les articles de cette sources parlent ainsi que le nombre d’articles et le ton moyen des articles (pour chaque thème/personne/lieu); permettez une agrégation par jour/mois/année.</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083134_-912516035","id":"20200118-171547_1094664052","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3447"},{"text":"// #####################\n// ### PREPROCESSING ###\n// #####################\n\n//FONCTION pour couper DATE en Year,month,day\n\nval slice = udf((string : String, from : Int, to : Int) => string.slice(from,to))\n\n//TABLE V2PERSONS\n\nval question3Df_person= gkgDF_selected.select(\"SourceCommonName\",\"DATE\",\"DocumentIdentifier\",\"V2Persons\",\"V2Tone\")\n.filter($\"SourceCommonName\".isNotNull)\n.filter($\"V2Persons\".isNotNull)\n.withColumn(\"V2Tone_split\",split($\"V2Tone\", \",\").getItem(0))\n.withColumn(\"V2Tone_float\",col(\"V2Tone_split\").cast(\"Float\"))\n.filter($\"V2Tone_float\".isNotNull)\n.withColumn(\"Persons_split\", split($\"V2Persons\", \";\"))\n.withColumn(\"Persons_unique\",explode($\"Persons_split\"))\n.withColumn(\"Persons\", split($\"Persons_unique\", \",\").getItem(0))\n.withColumn(\"YEAR\", slice($\"DATE\",lit(0),lit(4)))\n.withColumn(\"MONTH\", slice($\"DATE\",lit(4),lit(6)))\n.withColumn(\"DAY\", slice($\"DATE\",lit(6),lit(8)))\n.withColumn(\"date\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\",$\"DAY\"))\n.withColumn(\"mois_annee\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\"))\n.filter($\"Persons\" !== \"\")\n.drop(\"MONTH\")\n.drop($\"DAY\")\n.drop($\"DATE\")\n.drop($\"Persons_split\")\n.drop($\"Persons_unique\")\n.drop($\"V2Persons\")\n\n\n//TABLE V2THEMES\n\nval question3Df_themes = gkgDF_selected.select(\"SourceCommonName\",\"DATE\",\"DocumentIdentifier\",\"V2Themes\",\"V2Tone\")\n.filter($\"SourceCommonName\".isNotNull)\n.filter($\"V2Themes\".isNotNull)\n.withColumn(\"V2Tone_split\",split($\"V2Tone\", \",\").getItem(0))\n.withColumn(\"V2Tone_float\",col(\"V2Tone_split\").cast(\"Float\"))\n.filter($\"V2Tone_float\".isNotNull)\n.withColumn(\"Themes_split\", split($\"V2Themes\", \";\"))\n.withColumn(\"Themes_unique\",explode($\"Themes_split\"))\n.withColumn(\"Themes\", split($\"Themes_unique\", \",\").getItem(0))\n.withColumn(\"YEAR\", slice($\"DATE\",lit(0),lit(4)))\n.withColumn(\"MONTH\", slice($\"DATE\",lit(4),lit(6)))\n.withColumn(\"DAY\", slice($\"DATE\",lit(6),lit(8)))\n.withColumn(\"date\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\",$\"DAY\"))\n.withColumn(\"mois_annee\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\"))\n.filter($\"Themes\" !== \"\")\n.drop(\"MONTH\")\n.drop($\"DAY\")\n.drop($\"DATE\")\n.drop($\"Themes_split\")\n.drop($\"Themes_unique\")\n.drop($\"V2Themes\")\n\n\n//TABLE V2LOCATION\n\nval question3Df_locations = gkgDF_selected.select(\"SourceCommonName\",\"DATE\",\"DocumentIdentifier\",\"V2Locations\",\"V2Tone\")\n.filter($\"SourceCommonName\".isNotNull)\n.filter($\"V2Locations\".isNotNull)\n.withColumn(\"V2Tone_split\",split($\"V2Tone\", \",\").getItem(0))\n.withColumn(\"V2Tone_float\",col(\"V2Tone_split\").cast(\"Float\"))\n.filter($\"V2Tone_float\".isNotNull)\n.withColumn(\"Locations_split\", split($\"V2Locations\", \";\"))\n.withColumn(\"locations_unique\",explode($\"Locations_split\"))\n.withColumn(\"Locations\", split($\"locations_unique\", \"#\").getItem(2))   //.withColumn(\"Locations\", split($\"Locations_full\", \",\").getItem(0))\n.withColumn(\"YEAR\", slice($\"DATE\",lit(0),lit(4)))\n.withColumn(\"MONTH\", slice($\"DATE\",lit(4),lit(6)))\n.withColumn(\"DAY\", slice($\"DATE\",lit(6),lit(8)))\n.withColumn(\"date\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\",$\"DAY\"))\n.withColumn(\"mois_annee\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\"))\n.filter($\"Locations\" !== \"\")\n.drop(\"MONTH\")\n.drop($\"DAY\")\n.drop($\"DATE\")\n.drop($\"V2Locations\")\n.drop($\"Locations_split\")\n.drop($\"locations_unique\")\n.drop($\"Locations_full\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":117.798,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there were three deprecation warnings; re-run with -deprecation for details\nslice: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function3>,StringType,Some(List(StringType, IntegerType, IntegerType)))\nquestion3Df_person: org.apache.spark.sql.DataFrame = [SourceCommonName: string, date: string ... 7 more fields]\nquestion3Df_themes: org.apache.spark.sql.DataFrame = [SourceCommonName: string, date: string ... 7 more fields]\nquestion3Df_locations: org.apache.spark.sql.DataFrame = [SourceCommonName: string, date: string ... 7 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083134_319901990","id":"20200118-181236_1630432387","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3448"},{"text":"// PARTIE 1 - afficher les personnes, themes, lieux dont les articles de cette source parlent \n\n\n\n//1 - AFFICHER Les personnes dont les articles de cette source parle       \n\nval question3Df_person_collection= question3Df_person\n.groupBy(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\")\n.agg(collect_set(\"Persons\") as \"Persons_collection\")\n.sort($\"SourceCommonName\".desc)\n//.withColumn(\"no_of_Persons\", size($\"Persons_collection\"))\n\n\n//2 - AFFICHER Les themes dont les articles de cette source parle\n\nval question3Df_themes_collection = question3Df_themes\n.groupBy(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\")\n.agg(collect_set(\"Themes\") as \"Themes_collection\")\n.sort($\"SourceCommonName\".desc)\n//.withColumn(\"no_of_Themes\", size($\"Themes_collection\"))\n\n\n//3 - AFFICHER Les lieux dont les articles de cette source parle\n\nval question3Df_locations_collection = question3Df_locations\n.groupBy(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\")\n.agg(collect_set(\"Locations\") as \"Locations_collection\")\n.sort($\"SourceCommonName\".desc)\n//.withColumn(\"no_of_Locations\", size($\"Locations_collection\"))\n\n\n\n//UNION DES TABLES CI-DESSUS                                                                            \n\nval question3Df_person_themes_locations= question3Df_person_collection\n.join(question3Df_themes_collection, Seq(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\"))\n.join(question3Df_locations_collection, Seq(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\"))\n.withColumnRenamed(\"SourceCommonName\",\"source\")\n.withColumnRenamed(\"date\",\"date\")\n.withColumnRenamed(\"mois_annee\",\"mois_annee\")\n.withColumnRenamed(\"Year\", \"annee\")\n.withColumnRenamed(\"Persons_collection\",\"collection_personnes\")\n.withColumnRenamed(\"Themes_collection\",\"collection_themes\")\n.withColumnRenamed(\"Locations_collection\",\"collection_lieux\")\n//.withColumnRenamed(\"no_of_Persons\",\"nombre_personnes\")\n//.withColumnRenamed(\"no_of_Themes\",\"nombre_themes\")\n//.withColumnRenamed(\"no_of_Locations\",\"nombre_lieux\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"question3Df_person_collection: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [SourceCommonName: string, date: string ... 3 more fields]\nquestion3Df_themes_collection: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [SourceCommonName: string, date: string ... 3 more fields]\nquestion3Df_locations_collection: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [SourceCommonName: string, date: string ... 3 more fields]\nquestion3Df_person_themes_locations: org.apache.spark.sql.DataFrame = [source: string, date: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083134_-985863910","id":"20200119-121710_1364750513","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3449"},{"text":"//CASSANDRA: première partie de la requête - Personnes & Themes & Lieux\n\nquestion3Df_person_themes_locations.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question3Df_themes_collection_1mois.parquet\")\n\n// for 1 month\n//question3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question3Df_person_themes_locations.parquet\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1581022083134_-750778164","id":"20200123-130705_1790745487","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3450"},{"text":"spark.read.parquet(\"\"s3a://hiroto-yamakawa-telecom-gdelt2019/question3Df_person_themes_locations_6mois.parquet\")","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1581022083134_-1796378927","id":"20200124-081230_1443266979","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3451"},{"text":"// PARTIE 2    \n\n\n//afficher le nombre d'article par personne et le ton moyen des article                                                                                                                     \n\nval question3Df_persons_collection_bis= question3Df_person\n.groupBy(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\",\"Persons\")\n.agg(collect_set(\"DocumentIdentifier\") as \"Articles_collection\", mean(\"V2Tone_float\").as(\"Tone_mean\"))\n.withColumn(\"no_of_Article_person\", size($\"Articles_collection\"))\n.withColumnRenamed(\"SourceCommonName\",\"source\")\n.withColumnRenamed(\"date\", \"date\")\n.withColumnRenamed(\"mois_annee\", \"mois_annee\")\n.withColumnRenamed(\"Year\", \"annee\")\n.withColumnRenamed(\"Persons\",\"personne\")\n.withColumnRenamed(\"Tone_mean\",\"ton_moyen\")\n.withColumnRenamed(\"no_of_Article_person\",\"nombre_articles_personne\")\n.sort($\"nombre_articles_personne\".desc)\n.drop($\"Articles_collection\")\n\n\n//afficher le nombre d'article par themes et le ton moyen des article\n\nval question3Df_themes_collection_bis= question3Df_themes\n.groupBy(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\",\"Themes\")\n.agg(collect_set(\"DocumentIdentifier\") as \"Articles_collection\", mean(\"V2Tone_float\").as(\"Tone_mean\"))\n.withColumn(\"no_of_Article_theme\", size($\"Articles_collection\"))\n.withColumnRenamed(\"SourceCommonName\",\"source\")\n.withColumnRenamed(\"date\", \"date\")\n.withColumnRenamed(\"mois_annee\", \"mois_annee\")\n.withColumnRenamed(\"Year\", \"annee\")\n.withColumnRenamed(\"Themes\",\"theme\")\n.withColumnRenamed(\"Tone_mean\",\"ton_moyen\")\n.withColumnRenamed(\"no_of_Article_theme\",\"nombre_articles_theme\")\n.sort($\"nombre_articles_theme\".desc)\n.drop($\"Articles_collection\")\n\n//afficher le nombre d'article par lieu et le ton moyen des article\n\nval question3Df_locations_collection_bis= question3Df_locations\n.groupBy(\"SourceCommonName\",\"date\",\"mois_annee\",\"Year\",\"Locations\")\n.agg(collect_set(\"DocumentIdentifier\") as \"Articles_collection\", mean(\"V2Tone_float\").as(\"Tone_mean\"))\n.withColumn(\"no_of_Article_location\", size($\"Articles_collection\"))\n.withColumnRenamed(\"SourceCommonName\",\"source\")\n.withColumnRenamed(\"date\", \"date\")\n.withColumnRenamed(\"mois_annee\", \"mois_annee\")\n.withColumnRenamed(\"Year\", \"annee\")\n.withColumnRenamed(\"Locations\",\"lieu\")\n.withColumnRenamed(\"Tone_mean\",\"ton_moyen\")\n.withColumnRenamed(\"no_of_Article_location\",\"nombre_articles_lieu\")\n.sort($\"nombre_articles_lieu\".desc)\n.drop($\"Articles_collection\")\n\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"question3Df_persons_collection_bis: org.apache.spark.sql.DataFrame = [source: string, date: string ... 5 more fields]\nquestion3Df_themes_collection_bis: org.apache.spark.sql.DataFrame = [source: string, date: string ... 5 more fields]\nquestion3Df_locations_collection_bis: org.apache.spark.sql.DataFrame = [source: string, date: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083134_-1108615604","id":"20200119-113813_1306004844","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3452"},{"text":"//CASSANDRA: deuxieme partie de la requete - personne\nquestion3Df_persons_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question3Df_persons_collection_6mois.parquet\")\n\n// for 1 month\n//question3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question3Df_persons_collection.parquet\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1581022083135_1183468528","id":"20200123-131113_1067777425","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3453"},{"text":"//CASSANDRA: deuxieme partie de la requete - theme\nquestion3Df_themes_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question3Df_themes_collection_6mois.parquet\")\n\n// for 1 month\n//question3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question3Df_themes_collection.parquet\")\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1581022083135_814739759","id":"20200123-155345_1652285085","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3454"},{"text":"//CASSANDRA: deuxieme partie de la requete - location\nquestion3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question3Df_locations_collection_6mois1.parquet\")\n\n// for 1 month\n//question3Df_locations_collection_bis.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question3Df_locations_collection.parquet\")","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1581022083135_1151889753","id":"20200123-155353_527268317","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3455"},{"text":"val test2 = spark.read.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question3Df_locations_collection_6mois1.parquet\")\n\ntest2.filter($\"lieu\" === \"US\").show()","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+----------+----------+-----+----+--------------------+--------------------+\n|           source|      date|mois_annee|annee|lieu|           ton_moyen|nombre_articles_lieu|\n+-----------------+----------+----------+-----+----+--------------------+--------------------+\n|   1019thekeg.com|2019/11/07|   2019/11| 2019|  US| -0.6681534051895142|                   4|\n|   1021thefox.com|2019/07/18|   2019/07| 2019|  US|  1.6474293271700542|                   4|\n|      1023xlc.com|2019/11/13|   2019/11| 2019|  US| -1.9181906418366865|                   4|\n|     1025kiss.com|2019/09/12|   2019/09| 2019|  US| -1.6444383817059653|                   4|\n|1045freshradio.ca|2019/09/29|   2019/09| 2019|  US| -3.2133610546588898|                   4|\n|    1075zoofm.com|2019/08/16|   2019/08| 2019|  US|    1.62060238014568|                   4|\n|     1630kcjj.com|2019/12/04|   2019/12| 2019|  US|  1.6564164331981115|                   4|\n|       180.com.uy|2019/08/14|   2019/08| 2019|  US| 0.18765061497688293|                   4|\n|      24matins.fr|2019/11/16|   2019/11| 2019|  US| -0.6621982024775611|                   4|\n|  2paragraphs.com|2019/10/22|   2019/10| 2019|  US|   -1.72341445514134|                   4|\n|         5280.com|2019/09/17|   2019/09| 2019|  US|  1.9002807917016926|                   4|\n|     7thspace.com|2019/07/13|   2019/07| 2019|  US|  -5.576736238267687|                   4|\n| 900theticket.com|2019/07/26|   2019/07| 2019|  US| -2.3701049238443375|                   4|\n|           902.gr|2019/10/21|   2019/10| 2019|  US|  -1.259465180337429|                   4|\n|    929jackfm.com|2019/12/18|   2019/12| 2019|  US| -0.4269070625305176|                   4|\n| 929theticket.com|2019/07/09|   2019/07| 2019|  US|  1.7396890434526628|                   4|\n| 92qnashville.com|2019/10/03|   2019/10| 2019|  US|  0.9755021333694458|                   4|\n| 92qnashville.com|2019/10/31|   2019/10| 2019|  US|-0.27688951293627423|                   4|\n|  937kcountry.com|2019/08/09|   2019/08| 2019|  US|  1.2536523342132568|                   4|\n|  991thewhale.com|2019/11/23|   2019/11| 2019|  US|  -5.669776376555948|                   4|\n+-----------------+----------+----------+-----+----+--------------------+--------------------+\nonly showing top 20 rows\n\ntest2: org.apache.spark.sql.DataFrame = [source: string, date: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083135_1404087915","id":"20200124-083414_1894085579","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3456"},{"text":"%md \n### 4) dresser la cartographie des relations entre les pays d’après le ton des articles : pour chaque paire (pays1, pays2), calculer le nombre d’article, le ton moyen (aggrégations sur Année/Mois/Jour, filtrage par pays ou carré de coordonnées)","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>4) dresser la cartographie des relations entre les pays d’après le ton des articles : pour chaque paire (pays1, pays2), calculer le nombre d’article, le ton moyen (aggrégations sur Année/Mois/Jour, filtrage par pays ou carré de coordonnées)</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1581022083135_-893522667","id":"20200118-171733_1899814154","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3457"},{"text":"//fonction pour couper la date\nval slice = udf((string : String, from : Int, to : Int) => string.slice(from,to))\n\n\nval question4Df = event_mention_DF.select(\"Actor1CountryCode\",\"Actor2CountryCode\",\"GLOBALEVENTID\",\"MentionTimeDate\",\"MentionIdentifier\",\"MentionDocTone\")\n.filter($\"Actor1CountryCode\" !== \"\")\n.filter($\"Actor2CountryCode\" !== \"\")\n.filter($\"Actor1CountryCode\" !== $\"Actor2CountryCode\")       \n.withColumn(\"CountryPair\", array(event_mention_DF(\"Actor1CountryCode\"), event_mention_DF(\"Actor2CountryCode\")))\n.withColumn(\"CountryPair_sorted\", sort_array($\"CountryPair\"))// to avoid having [USA,FRANCE] different from [FRANCE,USA]\n.withColumnRenamed(\"Actor1CountryCode\", \"pays1\")\n.withColumnRenamed(\"Actor2CountryCode\", \"pays2\")\n.withColumn(\"YEAR\", slice($\"MentionTimeDate\",lit(0),lit(4)))\n.withColumn(\"MONTH\", slice($\"MentionTimeDate\",lit(4),lit(6)))\n.withColumn(\"DAY\", slice($\"MentionTimeDate\",lit(6),lit(8)))\n.withColumn(\"date\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\",$\"DAY\"))\n.withColumn(\"mois_annee\", concat_ws(\"/\",$\"YEAR\",$\"MONTH\"))\n.withColumnRenamed(\"YEAR\", \"annee\")\n.withColumnRenamed(\"CountryPair_sorted\", \"pair_pays\")                                                                             //FIPS10-4 Country code dictionnary\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.drop($\"MONTH\")\n.drop($\"DAY\")\n.drop($\"MentionTimeDate\")\n\n\n\nval question4DfFinal = question4Df\n.groupBy(\"pair_pays\",\"date\",\"mois_annee\",\"annee\")\n.agg(countDistinct($\"MentionIdentifier\") as \"nombre_article\", mean(\"MentionDocTone\").as(\"ton_moyen\"))\n\n","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there were three deprecation warnings; re-run with -deprecation for details\nslice: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function3>,StringType,Some(List(StringType, IntegerType, IntegerType)))\nquestion4Df: org.apache.spark.sql.DataFrame = [pays1: string, pays2: string ... 8 more fields]\nquestion4DfFinal: org.apache.spark.sql.DataFrame = [pair_pays: array<string>, date: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083135_-324999628","id":"20200119-134350_964253015","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3458"},{"text":"//CASSANDRA\nquestion4DfFinal.write.parquet(\"s3a://hiroto-yamakawa-gdelt2019/question4Df_6mois.parquet\")\n\n// for 1 month\n//question4DfFinal.write.parquet(\"s3a://hiroto-yamakawa-telecom-gdelt2019/question4Df_1mois_corrige.parquet\")","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:83)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n  ... 53 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 26.0 failed 4 times, most recent failure: Lost task 19.3 in stage 26.0 (TID 20030, ip-172-31-7-74.ec2.internal, executor 12): java.io.EOFException: Unexpected end of ZLIB input stream\n\tat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n\tat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n\tat java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n\tat scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n\tat scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:115)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n  ... 75 more\nCaused by: java.io.EOFException: Unexpected end of ZLIB input stream\n  at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n  at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n  at java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n  at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n  at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n  at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n  at java.io.InputStreamReader.read(InputStreamReader.java:184)\n  at java.io.BufferedReader.fill(BufferedReader.java:161)\n  at java.io.BufferedReader.readLine(BufferedReader.java:324)\n  at java.io.BufferedReader.readLine(BufferedReader.java:389)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:61)\n  at scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n  at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n  at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:115)\n  at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:107)\n  at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1581022083135_-845551162","id":"20200123-125120_204766638","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3459"},{"text":"// ###################\n// #### COMMENTS #####\n// ###################\n\n\n//justification entre utilisation de COUNT VS COUNTDISTINCT   et CountryPair VS CountryPair_sorted\n\n//EN UTILISANT COUNT (nombre de mentions),  avec [USA,FRANCE] different de [FRANCE,USA]\n\nval question4Df_year_unsorted= question4Df\n.groupBy(\"CountryPair\",\"YEAR\")\n.agg(count($\"MentionIdentifier\") as \"distinct_count\", mean(\"MentionDocTone\").as(\"Tone_mean\"))\n.withColumnRenamed(\"YEAR\", \"annee\")\n.withColumnRenamed(\"CountryPair\", \"pair_pays\")                                                                             //FIPS10-4 Country code dictionnary\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.withColumnRenamed(\"distinct_count\", \"nombre_article\")\n\nquestion4Df_year_unsorted.where(array_contains($\"pair_pays\", \"CAN\")).where(array_contains($\"pair_pays\", \"USA\")).show()\n\n\n//EN UTILISANT COUNT(nombre de mentions) ,  avec [USA,FRANCE] = [FRANCE,USA] \nval question4Df_year_sorted= question4Df\n.groupBy(\"CountryPair_sorted\",\"YEAR\")\n.agg(count($\"MentionIdentifier\") as \"distinct_count\", mean(\"MentionDocTone\").as(\"Tone_mean\"))\n.withColumnRenamed(\"YEAR\", \"annee\")\n.withColumnRenamed(\"CountryPair_sorted\", \"pair_pays\")                                                                             //FIPS10-4 Country code dictionnary\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.withColumnRenamed(\"distinct_count\", \"nombre_article\")\n\nquestion4Df_year_sorted.where(array_contains($\"pair_pays\", \"CAN\")).where(array_contains($\"pair_pays\", \"USA\")).show()\n\n//EN UTILISANT COUNTDISTINCT (nombre d'articles), avec [USA,FRANCE] different de [FRANCE,USA]\n\nval question4Df_year_unsorted_distinct = question4Df\n.groupBy(\"CountryPair\",\"YEAR\")\n.agg(countDistinct($\"MentionIdentifier\") as \"distinct_count\", mean(\"MentionDocTone\").as(\"Tone_mean\"))\n.withColumnRenamed(\"YEAR\", \"annee\")\n.withColumnRenamed(\"CountryPair\", \"pair_pays\")                                                                             //FIPS10-4 Country code dictionnary\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.withColumnRenamed(\"distinct_count\", \"nombre_article\")\n\nquestion4Df_year_unsorted_distinct.where(array_contains($\"pair_pays\", \"CAN\")).where(array_contains($\"pair_pays\", \"USA\")).show()\n\n\n//EN UTILISANT COUNTDISTINCT (nombre d'articles), avec [USA,FRANCE] == [FRANCE,USA]                                                                                                                            //CELUI QU'ON DEVRAIT UTILISER\nval question4Df_year_sorted_distinct= question4Df\n.groupBy(\"CountryPair_sorted\",\"YEAR\")\n.agg(countDistinct($\"MentionIdentifier\") as \"distinct_count\", mean(\"MentionDocTone\").as(\"Tone_mean\"))\n.withColumnRenamed(\"YEAR\", \"annee\")\n.withColumnRenamed(\"CountryPair_sorted\", \"pair_pays\")                                                                             //FIPS10-4 Country code dictionnary\n.withColumnRenamed(\"GLOBALEVENTID\", \"evenement\")\n.withColumnRenamed(\"distinct_count\", \"nombre_article\")\n\nquestion4Df_year_sorted_distinct.where(array_contains($\"pair_pays\", \"CAN\")).where(array_contains($\"pair_pays\", \"USA\")).show()","user":"anonymous","dateUpdated":"2020-02-06T20:48:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+--------------+-------------------+\n| pair_pays|annee|nombre_article|          Tone_mean|\n+----------+-----+--------------+-------------------+\n|[CAN, USA]| 2018|           837|-0.8449978254760903|\n|[USA, CAN]| 2018|           769|-0.6131089314596184|\n+----------+-----+--------------+-------------------+\n\n+----------+-----+--------------+-------------------+\n| pair_pays|annee|nombre_article|          Tone_mean|\n+----------+-----+--------------+-------------------+\n|[CAN, USA]| 2018|          1606|-0.7339626078555008|\n+----------+-----+--------------+-------------------+\n\n+----------+-----+--------------+-------------------+\n| pair_pays|annee|nombre_article|          Tone_mean|\n+----------+-----+--------------+-------------------+\n|[CAN, USA]| 2018|           327|-0.8449978254760905|\n|[USA, CAN]| 2018|           236| -0.613108931459619|\n+----------+-----+--------------+-------------------+\n\n+----------+-----+--------------+-------------------+\n| pair_pays|annee|nombre_article|          Tone_mean|\n+----------+-----+--------------+-------------------+\n|[CAN, USA]| 2018|           381|-0.7339626078555014|\n+----------+-----+--------------+-------------------+\n\nquestion4Df_year_unsorted: org.apache.spark.sql.DataFrame = [pair_pays: array<string>, annee: string ... 2 more fields]\nquestion4Df_year_sorted: org.apache.spark.sql.DataFrame = [pair_pays: array<string>, annee: string ... 2 more fields]\nquestion4Df_year_unsorted_distinct: org.apache.spark.sql.DataFrame = [pair_pays: array<string>, annee: string ... 2 more fields]\nquestion4Df_year_sorted_distinct: org.apache.spark.sql.DataFrame = [pair_pays: array<string>, annee: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1581022083136_-1562568383","id":"20200119-141116_1091176903","dateCreated":"2020-02-06T20:48:03+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3460"}],"name":"gdeltExploration (S3 + 6 mois)","id":"2F34HR651","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}